zookeeper -> hdfs -> hbase -> thrift -> spark

bin/spark-submit --master spark://bigdata111:7077 --class org.apache.spark.examples.SparkPi
examples/jars/spark-examples_2.11-2.1.0.jar 100

# local mode 
  bin/spark-shell
  
# cluster mode
  bin/spark-shell --master spark://bigdata111:7077
  
  bin/pyspark
  
    >>>host= '127.0.0.1'

    >>>inputtable= 'hbase_1102'

    >>>hbaseconf={"hbase.zookeeper.quorum":host,"hbase.mapreduce.inputtable":inputtable}

    >>>keyConv="org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter"

    >>>valueConv="org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter"

    >>>hbase_rdd=sc.newAPIHadoopRDD("org.apache.hadoop.hbase.mapreduce.TableInputFormat","org.apache.hadoop.hbase.io.ImmutableBytesWritable","org.apache.hadoop.hbase.client.Result",keyConverter=keyConv,valueConverter=valueConv,conf=hbaseconf)

    >>>hbase_rdd.count()
