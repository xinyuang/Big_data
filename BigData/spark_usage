zookeeper -> hdfs -> hbase -> thrift -> spark

bin/spark-submit --master spark://bigdata111:7077 --class org.apache.spark.examples.SparkPi
examples/jars/spark-examples_2.11-2.1.0.jar 100

bin/spark-submit --master spark://bigdata111:7077 --class demo.JavaSparkWC2 /root/test/javaSparkWC.jar hdfs://192.168.137.111:9000/user/root/input/myword.txt


# local mode 
  bin/spark-shell
  
# cluster mode
  bin/spark-shell --master spark://bigdata111:7077
  
  bin/pyspark
  
    >>>host= '192.168.137.111'

    >>>inputtable= 'traxen_test'

    >>>hbaseconf={"hbase.zookeeper.quorum":host,"hbase.mapreduce.inputtable":inputtable}

    >>>keyConv="org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter"

    >>>valueConv="org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter"

    >>>hbase_rdd=sc.newAPIHadoopRDD("org.apache.hadoop.hbase.mapreduce.TableInputFormat","org.apache.hadoop.hbase.io.ImmutableBytesWritable","org.apache.hadoop.hbase.client.Result",keyConverter=keyConv,valueConverter=valueConv,conf=hbaseconf)

    >>>hbase_rdd.count()
        
    >>>hbase_rdd.cache()
    >>>output = hbase_rdd.collect()
    >>>for (k, v) in output:
    >>>       print (k, v)

from pyspark.sql import SparkSession
from pyspark import SparkContext,SparkConf
 
spark=SparkSession.builder.appName("abv").getOrCreate() 
print('spark sc created')
host = 'learn'
table = 'student'
keyConv = "org.apache.spark.examples.pythonconverters.StringToImmutableBytesWritableConverter"
valueConv = "org.apache.spark.examples.pythonconverters.StringListToPutConverter"
conf = {"hbase.zookeeper.quorum": host,"hbase.mapred.outputtable": table,"mapreduce.outputformat.class": "org.apache.hadoop.hbase.mapreduce.TableOutputFormat","mapreduce.job.output.key.class": "org.apache.hadoop.hbase.io.ImmutableBytesWritable","mapreduce.job.output.value.class": "org.apache.hadoop.io.Writable"}
 
rawData = ['3,info,name,Rongcheng','4,info,name,Guanhua']
#( rowkey , [ row key , column family , column name , value ] )
print('write into hbase')
spark.sparkContext.parallelize(rawData).map(lambda x: (x[0],x.split(','))).saveAsNewAPIHadoopDataset(conf=conf,keyConverter=keyConv,valueConverter=valueConv)

  # hive spark
  scala> spark.sql("show tables").show
