zookeeper -> hdfs -> hbase -> thrift -> spark

bin/spark-submit --master spark://bigdata111:7077 --class org.apache.spark.examples.SparkPi
examples/jars/spark-examples_2.11-2.1.0.jar 100

bin/spark-submit --master spark://bigdata111:7077 --class demo.JavaSparkWC2 /root/test/javaSparkWC.jar hdfs://192.168.137.111:9000/user/root/input/myword.txt


# local mode 
  bin/spark-shell
  
# cluster mode
  bin/spark-shell --master spark://bigdata111:7077
  
  bin/pyspark
  
    >>>host= '192.168.137.111'

    >>>inputtable= 'traxen_test'

    >>>hbaseconf={"hbase.zookeeper.quorum":host,"hbase.mapreduce.inputtable":inputtable}

    >>>keyConv="org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter"

    >>>valueConv="org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter"

    >>>hbase_rdd=sc.newAPIHadoopRDD("org.apache.hadoop.hbase.mapreduce.TableInputFormat","org.apache.hadoop.hbase.io.ImmutableBytesWritable","org.apache.hadoop.hbase.client.Result",keyConverter=keyConv,valueConverter=valueConv,conf=hbaseconf)

    >>>hbase_rdd.count()
        
    >>>hbase_rdd.cache()
    >>>output = hbase_rdd.collect()
    >>>for (k, v) in output:
    >>>       print (k, v)

from pyspark.sql import SparkSession
from pyspark import SparkContext,SparkConf
 
spark=SparkSession.builder.appName("abv").getOrCreate() 
print('spark sc created')
host = 'learn'
table = 'student'
keyConv = "org.apache.spark.examples.pythonconverters.StringToImmutableBytesWritableConverter"
valueConv = "org.apache.spark.examples.pythonconverters.StringListToPutConverter"
conf = {"hbase.zookeeper.quorum": host,"hbase.mapred.outputtable": table,"mapreduce.outputformat.class": "org.apache.hadoop.hbase.mapreduce.TableOutputFormat","mapreduce.job.output.key.class": "org.apache.hadoop.hbase.io.ImmutableBytesWritable","mapreduce.job.output.value.class": "org.apache.hadoop.io.Writable"}
 
rawData = ['3,info,name,Rongcheng','4,info,name,Guanhua']
#( rowkey , [ row key , column family , column name , value ] )
print('write into hbase')
spark.sparkContext.parallelize(rawData).map(lambda x: (x[0],x.split(','))).saveAsNewAPIHadoopDataset(conf=conf,keyConverter=keyConv,valueConverter=valueConv)


 import org.apache.spark.sql.{SQLContext, _}
 import org.apache.spark.sql.execution.datasources.hbase._
 import org.apache.spark.{SparkConf, SparkContext}
 import spark.sqlContext.implicits._
 import org.apache.spark.sql.datasources.hbase.{HBaseTableCatalog}

scala> def catalog = s"""{
     | "table":{"namespace":"traxen", "name":"test"},
     | "rowkey":"key",
     | "columns":{
     | "rowkey":{"cf":"rowkey", "col":"key", "type":"string"},
     | "logGPS":{"cf":"log","col":"gps","type":"string"}，
     | "logSpeed":{"cf":"log","col":"speed","type":"string"}，
     | "logInfo":{"cf":"log","col":"info","type":"string"}
     | }
     | }""".stripMargin




  # hive spark
  scala> spark.sql("show tables").show
  scala> spark.sql("create table emp (empno int, ename string, job string, mgr string, hiredate string,sal int, comm int) row format delimited fields terminated by ','")
  scala> spark.sql("load data inpath '/csv_data/emp' into table emp")
  scala> val DF1 = spark.sql("select * from emp") 
  scala> DF1.registerTempTable("tem_emp")
  scala> spark.sqlContext.cacheTable("tem_emp")
  scala> spark.sql("select * from tem_emp").show
  scala> spark.sqlContext.clearCache  

  
