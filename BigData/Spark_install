tar -zxvf spark-2.1.0-bin-hadoop2.7.tgz -C ../training/

cp spark-env.sh.template spark-env.sh

vi conf/spark-env.sh
  export JAVA_HOME=/root/training/jdk1.8.0_144

# HA for standalone mode
  export SPARK_MASTER_HOST=bigdata111
  export SPARK_MASTER_PORT=7077
# mkdir recovery
  export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=FILESYSTEM
  -Dspark.deploy.recoveryDirectory=/root/training/spark-2.1.0-bin-hadoop2.7/recovery"
    
# HA use zookeeper
  # make sure /etc/hostname is bigdata111,bigdata112,bigdata113 individually
  export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER 
  -Dspark.deploy.zookeeper.url=bigdata111,bigdata112,bigdata113 -Dspark.deploy.zookeeper.dir=/spark"
  export SPARK_MASTER_PORT=7077


    sbin/start-master.sh (start standby master manually)
  
cp slaves.template slaves

vi conf/slaves
  bigdata112
  bigdata113

sbin/start-all.sh

# python connect hbase to spark  
pip install pyspark

mkdir /root/training/spark-2.1.0-bin-hadoop2.7/jars/hbase
cd /root/training/spark-2.1.0-bin-hadoop2.7/jars/hbase # copy hbase jars file to spark/jars/hbase
    cp $HBASE_HOME/lib/hbase*.jar ./

    cp $HBASE_HOME/lib/guava-12.0.1.jar ./

    cp $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar ./

    cp $HBASE_HOME/lib/protobuf-java-2.5.0.jar ./

    cp $HBASE_HOME/lib/metrics-core-*.jar ./
    
download spark-examples_2.11-1.6.0-typesafe-001.jar to /root/training/spark-2.1.0-bin-hadoop2.7/jars/hbase
    
vi conf/spark-env.sh
  export SPARK_DIST_CLASSPATH=/root/training/spark-2.1.0-bin-hadoop2.7/jars/hbase/*
