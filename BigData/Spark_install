tar -zxvf spark-2.1.0-bin-hadoop2.7.tgz -C ../training/

sudo nano ~/.profile
    export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native/:$LD_LIBRARY_PATH

cp spark-env.sh.template spark-env.sh

vi conf/spark-env.sh
  export JAVA_HOME=/root/training/jdk1.8.0_144

# HA for standalone mode
  export SPARK_MASTER_HOST=bigdata111
  export SPARK_MASTER_PORT=7077
# mkdir recovery
  export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=FILESYSTEM
  -Dspark.deploy.recoveryDirectory=/root/training/spark-2.1.0-bin-hadoop2.7/recovery"
    
# HA use zookeeper
  # make sure /etc/hostname is bigdata111,bigdata112,bigdata113 individually
  export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER 
  -Dspark.deploy.zookeeper.url=bigdata111,bigdata112,bigdata113 -Dspark.deploy.zookeeper.dir=/spark"
  export SPARK_MASTER_PORT=7077


    sbin/start-master.sh (start standby master manually)
  
cp slaves.template slaves

vi conf/slaves
  bigdata112
  bigdata113

sbin/start-all.sh

# connect hbase to spark  
pip install pyspark

mkdir /root/training/spark-2.1.0-bin-hadoop2.7/jars/hbase
cd /root/training/spark-2.1.0-bin-hadoop2.7/jars/hbase # copy hbase jars file to spark/jars/hbase
    cp $HBASE_HOME/lib/hbase*.jar ./

    cp $HBASE_HOME/lib/guava-12.0.1.jar ./

    cp $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar ./

    cp $HBASE_HOME/lib/protobuf-java-2.5.0.jar ./

    cp $HBASE_HOME/lib/metrics-core-*.jar ./
    
download spark-examples_2.11-1.6.0-typesafe-001.jar to /root/training/spark-2.1.0-bin-hadoop2.7/jars/hbase
download hbase-spark-2.0.0-20160316.173537-2.jar to /root/training/spark-2.1.0-bin-hadoop2.7/jars/hbase http://maven.wso2.org/nexus/content/repositories/Apache/org/apache/hbase/hbase-spark/2.0.0-SNAPSHOT/    
vi conf/spark-env.sh
  export SPARK_DIST_CLASSPATH= $SPARK_HOME/jars/hbase/*

# connect hive to spark

    sudo cp ~/training/apache-hive-2.3.0-bin/conf/hive-site.xml ~/training/spark-2.1.0-bin-hadoop2.7/conf/
    sudo cp ~/training/hadoop-2.7.3/etc/hadoop/core-site.xml ~/training/spark-2.1.0-bin-hadoop2.7/conf/
    sudo cp ~/training/hadoop-2.7.3/etc/hadoop/hdfs-site.xml ~/training/spark-2.1.0-bin-hadoop2.7/conf/
    sudo cp mysql-connector-java-5.1.47-bin.jar ~/training/spark-2.1.0-bin-hadoop2.7/jars/
    
    start-dfs.sh
    spark/sbin/start-all.sh
    


