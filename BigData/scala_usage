bin/spark-shell

http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html

List example
Key,Value exapmle
HDFS file checkpoint and cache
Aggregate example (partition with index)
sparkSQL

List example:

    scala> val rdd1 = sc.parallelize(Array(5,6,7,8,9,1,2,3))
    rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24

    scala> val rdd2 = rdd1.map(_*2).sortBy(x=>x,true)
    rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[6] at sortBy at <console>:26

    scala> rdd2.collect
    res0: Array[Int] = Array(2, 4, 6, 10, 12, 14, 16, 18)

    scala> val rdd3 = rdd2.filter(_>10)
    rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at filter at <console>:28

    scala> rdd3.collect
    res1: Array[Int] = Array(12, 14, 16, 18)

    scala> val rdd4 = sc.parallelize(List("a b c", "d e f","z x y"))
    rdd4: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[8] at parallelize at <console>:24

    scala> val rdd5 = rdd4.flatMap(_.split(" "))
    rdd5: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at flatMap at <console>:26

    scala> rdd5.collect
    res2: Array[String] = Array(a, b, c, d, e, f, z, x, y)

    scala> val rdd6 = sc.parallelize(List(5,6,7,8,9,2,34))
    rdd6: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at <console>:24

    scala> val rdd7 = sc.parallelize(List(1,2,3,4,5,6,7))
    rdd7: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at parallelize at <console>:24

    scala> val rdd8 = rdd6.union(rdd7).collect
    rdd8: Array[Int] = Array(5, 6, 7, 8, 9, 2, 34, 1, 2, 3, 4, 5, 6, 7)

    scala> val rdd9 = rdd6.intersection(rdd7).collect
    rdd9: Array[Int] = Array(6, 7, 5, 2)

Key,Value exapmle:

    scala> val rdd1 = sc.parallelize(List(("Tom",1000),("Jerry",3000),("Marry",2000)))
    rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[1] at parallelize at <console>:24

    scala> val rdd2 = sc.parallelize(List(("Mike",2000),("Jerry",1000),("Tom",3000)))
    rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[2] at parallelize at <console>:24

    scala> val rdd3 = rdd1 union rdd2  
      or
    scala> val rdd3 = rdd1.union(rdd2)
    rdd3: org.apache.spark.rdd.RDD[(String, Int)] = UnionRDD[4] at union at <console>:28

    scala> rdd3.collect
    res1: Array[(String, Int)] = Array((Tom,1000), (Jerry,3000), (Marry,2000), (Mike,2000), (Jerry,1000), (Tom,3000))

    scala> val rdd4 = rdd3.groupByKey.collect
    rdd4: Array[(String, Iterable[Int])] = Array((Mike,CompactBuffer(2000)), (Marry,CompactBuffer(2000)), (Jerry,CompactBuffer(3000, 1000)), (Tom,CompactBuffer(1000, 3000)))

    scala> val rdd5 = rdd1.cogroup(rdd2).collect
    rdd5: Array[(String, (Iterable[Int], Iterable[Int]))] = Array((Mike,(CompactBuffer(),CompactBuffer(2000))), (Tom,(CompactBuffer(1000),CompactBuffer(3000))), (Marry,(CompactBuffer(2000),CompactBuffer())), (Jerry,(CompactBuffer(3000),CompactBuffer(1000))))

    scala> val rdd6 = rdd3.reduceByKey(_+_).collect
    rdd4: Array[(String, Int)] = Array((Mike,2000), (Marry,2000), (Jerry,4000), (Tom,4000))

    scala> val rdd7 = rdd6.map(t=>(t._2,t._1)).sortByKey(false).map(t=>(t._2,t._1)).collect
    rdd7: Array[(String, Int)] = Array((Jerry,4000), (Tom,4000), (Mike,2000), (Marry,2000))

HDFS file checkpoint and cache
    scala> sc.setCheckpointDir("hdfs://bigdata111:9000/sparkckpt")
    scala> var rdd1 = sc.textFile("hdfs://bigdata111:9000/data")
    scala> rdd1.checkpoint
    scala> rdd1.cache
    scala> rdd1.count

Aggregate example
    scala> val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9),2)
    scala> def func1(index:Int,iter:Iterator[Int]):Iterator[String]={iter.toList.map(x=>"[PartID:"+index+",value="+x+"]").iterator}
    scala> rdd1.mapPartitionsWithIndex(func1).collect
    res0: Array[String] = Array([PartID:0,value=1], [PartID:0,value=2], [PartID:0,value=3], [PartID:0,value=4], [PartID:1,value=5], [PartID:1,value=6], [PartID:1,value=7], [PartID:1,value=8], [PartID:1,value=9])

    scala> val rdd2 = sc.parallelize(List(1,2,3,4,5),2)
    rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24
    scala> rdd2.mapPartitionsWithIndex(func1).collect
    res1: Array[String] = Array([PartID:0,value=1], [PartID:0,value=2], [PartID:1,value=3], [PartID:1,value=4], [PartID:1,value=5])
    scala> rdd2.aggregate(0)(math.max(_,_),_+_)
    res2: Int = 7
    scala> rdd2.aggregate(10)(math.max(_,_),_+_)  # 10 is the initial value
    res3: Int = 30
    scala> rdd2.aggregate(0)(_+_,_+_)
    res4: Int = 15
    scala> rdd2.aggregate(10)(_+_,_+_)
    res6: Int = 45
    
    scala> val rdd2 = sc.parallelize(List("a b c d e f", "e d f e s"),2).flatMap(_.split(" "))
    scala> def func2(index:Int,iter:Iterator[String]):Iterator[String]={iter.toList.map(x=>"[PartID:"+index+",value="+x+"]").iterator}
    scala> rdd2.mapPartitionsWithIndex(func2).collect
    res5: Array[String] = Array([PartID:0,value=a], [PartID:0,value=b], [PartID:0,value=c], [PartID:0,value=d], [PartID:0,value=e], [PartID:0,value=f], [PartID:1,value=e], [PartID:1,value=d], [PartID:1,value=f], [PartID:1,value=e], [PartID:1,value=s])   
    scala> rdd2.aggregate("")(_+_,_+_)
    res4: String = abcdefedfes
    scala> rdd2.aggregate("*")(_+_,_+_)
    res2: String = **abcdef*edfes


    scala> val pairRDD = sc.parallelize(List( ("cat",2), ("cat", 5), ("mouse", 4),("cat", 12), ("dog", 12), ("mouse", 2)), 2)
    scala> def func3(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = {
         |   iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
         | }
    scala> pairRDD.mapPartitionsWithIndex(func3).collect
    res5: Array[String] = Array([partID:0, val: (cat,2)], [partID:0, val: (cat,5)], [partID:0, val: (mouse,4)], [partID:1, val: (cat,12)], [partID:1, val: (dog,12)], [partID:1, val: (mouse,2)])
    scala> pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect
    res4: Array[(String, Int)] = Array((dog,12), (cat,17), (mouse,6))
    scala> pairRDD.aggregateByKey(0)(_+_, _ + _).collect
    res7: Array[(String, Int)] = Array((dog,12), (cat,19), (mouse,6))
    scala> pairRDD.reduceByKey(_+_).collect
    res8: Array[(String, Int)] = Array((dog,12), (cat,19), (mouse,6))

    scala> val rdd3 = rdd2.repartition(3)
    scala> val rdd4 = rdd2.coalesce(3,true)
    scala> rdd4.partitions.length
    res3: Int = 3

sparkSQL

    scala> case class Emp(empno:Int,ename:String,job:String,mgr:Int,hiredata:String,sal:Int,comm:Int,deptno:Int)
    defined class Emp
    scala> val lines=sc.textFile("hdfs://bigdata111:9000/csv_data/emp").map(_.split(","))
    lines: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:24
    scala> val allEmp=lines.map(x=>Emp(x(0).toInt,x(1),x(2),x(3).toInt,x(4),x(5).toInt,x(6).toInt,x(7).toInt))
    allEmp: org.apache.spark.rdd.RDD[Emp] = MapPartitionsRDD[3] at map at <console>:28
    scala> val df1 = allEmp.toDF
    df1: org.apache.spark.sql.DataFrame = [empno: int, ename: string ... 6 more fields]
    scala> df1.show
    +-----+------+---------+----+----------+----+----+------+
    |empno| ename|      job| mgr|  hiredata| sal|comm|deptno|
    +-----+------+---------+----+----------+----+----+------+
    | 7369| SMITH|    CLERK|7902|1980/12/17| 800|   0|    20|
    | 7499| ALLEN| SALESMAN|7698| 1981/2/20|1600| 300|    30|

    scala> val df = spark.read.json("/root/training/spark-2.1.0-bin-hadoop2.7/examples/src/main/resources/people.json").show
    scala> val df = spark.read.format("json").load("/root/training/spark-2.1.0-bin-hadoop2.7/examples/src/main/resources/people.json").show
    scala> df.printSchema
    scala> df1.select($"ename",$"sal",$"sal" + 100).show
    scala> df1.filter($"sal">2000).show
    scala> df1.groupBy($"deptno").count.show
    
    scala> df1.createOrReplaceTempView("emp")
    scala> spark.sql("select * from emp").show

    scala> case class Dept(deptno:Int,dname:String,loc:String)
    scala> val deptlines=sc.textFile("hdfs://bigdata111:9000/csv_data/dept").map(_.split(","))
    scala> val allDept = deptlines.map(x=>Dept(x(0).toInt,x(1),x(2)))
    scala> val df2 = allDept.toDF
    scala> df2.createOrReplaceTempView("dept")
    scala> spark.sql("select dname,ename from emp,dept where emp.deptno=dept.deptno").show








