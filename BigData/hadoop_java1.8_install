# install java1.8.0 JDK
    1. unzip JDK zipfile:  tar -zxvf jdk-8u144-linux-x64.tar.gz
    2. set environment variables:
        find current path of jdk: pwd
        (in CentOS) sudo vi ~/.bash_profile
        (in ubuntu) sudo vi ~/.profile
        
        add        
            JAVA_HOME=/home/xinyuang/test/Hadoop/jdk1.8.0_144
            export JAVA_HOME

            PATH=$JAVA_HOME/bin:$PATH
            export PATH
            
        update configuration: 
            (in CentOS) source ~/.bash_profile
            (in ubuntu) source ~/.profile 

# Turn off firewall
    service firewalld stop 
    systemctl disable firewalld

# Set host name
    sudo nano /etc/hosts
        192.168.19.101 Traxen-0005
        192.168.19.102 data2
        
# set SSH
    ssh-keygen -t rsa
    ssh-copy-id -i .ssh/id_rsa.pub xinyuang@Traxen-0005

# install Hadoop2.7.3 at NameNode
    1. unzip hadoop zipfile: tar -zxvf hadoop-2.7.3.tar.gz
    2. set environment variables:
         find current path of jdk: pwd
        (in CentOS) sudo vi ~/.bash_profile
        (in ubuntu) sudo vi ~/.profile   
        
        add
            HADOOP_HOME=/home/xinyuang/test/Hadoop/hadoop-2.7.3
            export HADOOP_HOME

            PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$PATH
            export PATH

# configure Hadoop nameNode
    1. set JAVA_HOME at 25 line
        echo $JAVA_HOME
        copy the path of JAVA_HOME
        cd Hadoop/hadoop-2.7.3/etc/hadoop
        sudo nano hadoop-env.sh
            export JAVA_HOME=/home/xinyuang/test/Hadoop/jdk1.8.0_144
            
    2. set replication and admin check
        sudo nano hdfs-site.xml
        add between <configuration> </configuration>
            <property>
                    <name>dfs.replication</name>
                    <value>2</value>
            </property>

            <property>
                    <name>dfs.permissions</name>
                    <value>false</value>
            </property>
            
            # set for HA (High Avalability)
                 # set namenodes
                    <property>
                        <name>dfs.nameservices</name>
                        <value>ns1</value>
                    </property>

                    <property>
                        <name>dfs.ha.namenodes.ns1</name>
                        <value>nn1,nn2</value>
                    </property>

                    <property>
                        <name>dfs.namenode.rpc-address.ns1.nn1</name>
                        <value>bigdata111:9000</value>
                    </property>

                    <property>
                        <name>dfs.namenode.rpc-address.ns1.nn2</name>
                        <value>bigdata112:9000</value>
                    </property>

                    <property>
                        <name>dfs.namenode.http-address.ns1.nn1</name>
                        <value>bigdata111:50070</value>
                    </property>

                    <property>
                        <name>dfs.namenode.http-address.ns1.nn2</name>
                        <value>bigdata112:50070</value>
                    </property>
   
                # set jounalnode and failover controller
                
                    <property>
                        <name>dfs.namenode.shared.edits.dir</name>
                        <value>qjournal://bigdata111:8485;bigdata112:8485;/ns1</value>
                    </property>

                    <property>
                        <name>dfs.journalnode.edits.dir</name>
                        <value>/root/training/hadoop-2.7.3/journal</value>
                    </property>

                   <property>
                        <name>dfs.ha.automatic-failover.enabled</name>
                        <value>true</value>
                    </property>

                    <property>
                        <name>dfs.client.failover.proxy.provider.ns1</name>
                        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
                    </property>

                # set isolation fence
                
                    <property>
                        <name>dfs.ha.fencing.methods</name>
                        <value>
                                sshfence
                                shell(/bin/true)
                        </value>
                    </property>

                    <property>
                        <name>dfs.ha.fencing.ssh.private-key-files</name>
                        <value>/root/.ssh/id_rsa</value>
                    </property>

                    <property>
                        <name>dfs.ha.fencing.ssh.connect-timeout</name>
                        <value>30000</value>
                    </property>

       
            <property>
                <name>dfs.namenode.name.dir</name>
                <value>/root/HDFS/dn</value>
            </property>

       
       # make sure hdfs have the right to access these dir
            <property>
               <name>dfs.data.dir</name>
               <value>/home/traxen/Hadoop/dn, /home/traxen/Hadoop2/dn</value>
           </property>

            
    3. set PRC port for NameNode and data direction
         sudo nano core-site.xml
         add between <configuration> </configuration>
       
           <property>
                    <name>fs.defaultFS</name>
                    <value>hdfs://Traxen-0005:9000</value>   
                    # for HA set multiple namenode
                    # <value>hdfs://ns1</value>
            </property>

            <property>
                    <name>hadoop.tmp.dir</name>
                    <value>/home/xinyuang/test/Hadoop/hadoop-2.7.3/test_data</value>
            </property>
            
            <property>
                <name>hadoop.proxyuser.root.hosts</name>
                <value>*</value>
            </property>

            <property>
                <name>hadoop.proxyuser.root.groups</name>
                <value>*</value>
            </property>
            
           <property>   # set zookeeper server for HA
                <name>ha.zookeeper.quorum</name>
                <value>bigdata111:2181,bigdata112:2181,bigdata113:2181</value>
           </property>



    4. set MR framework
         cp mapred-site.xml.template mapred-site.xml
         sudo nano mapred-site.xml
         add between <configuration> </configuration>
         <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
         </property>
         
     5. set Yarn resourcemanager IP and NodeManager execute way
          sudo nano yarn-site.xml
          add between <configuration> </configuration>
          <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>Traxen-0005</value>
          </property>
          
          # set for HA
                     <property>
                            <name>yarn.resourcemanager.ha.enabled</name>
                            <value>true</value>
                      </property>

                      <property>
                            <name>yarn.resourcemanager.cluster-id</name>
                            <value>yrc</value>
                      </property>

                      <property>
                            <name>yarn.resourcemanager.ha.rm-ids</name>
                            <value>rm1,rm2</value>
                      </property>

                      <property>
                            <name>yarn.resourcemanager.hostname.rm1</name>
                            <value>bigdata111</value>
                      </property>

                      <property>
                            <name>yarn.resourcemanager.hostname.rm2</name>
                            <value>bigdata112</value>
                      </property>

                      <property>
                            <name>yarn.resourcemanager.zk-address</name>
                            <value>bigdata111:2181,bigdata112:2181,bigdata113:2181</value>
                      </property>


          <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
          </property>
     6. set DataNode IP
          sudo nano slaves
          delete localhost
          add datanode IP

     7. format namenode
          hdfs namenode -format
          check log: Storage directory /home/xinyuang/test/Hadoop/hadoop-2.7.3/test_data/dfs/name has been successfully formatted.
   
# copy configured hadoop to dataNode
      scp -r hadoop-2.7.3/ root@bigdata112:/root/training/hadoop-2.7.3/
      
# start journalnode on assigned server
        hadoop-daemon.sh start journalnode
        
# copy active namenode log to standby namenode
        cd /root/training/hadoop-2.7.3/tmp_data
        scp -r dfs/ root@bigdata112:/root/training/hadoop-2.7.3/tmp_data
        
     8. format zookeeper
            hdfs zkfc -formatZK
            check log: INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/ns1 in ZK.
            
# start resource manager manually for standby namenode
      yarn-daemon.sh start resourcemanager
        
    


      


        
